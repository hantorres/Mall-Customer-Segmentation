{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986e1266",
   "metadata": {},
   "source": [
    "## Optimizing the Mall Customer Segmentation Model\n",
    "\n",
    "The efficiency of the model may be improved by employing strategies like efficient numerics representation and efficient model representation. These strategies will optimize memory usage and training time without significantly impacting model performance.\n",
    "\n",
    "First, I will import the original model to benchmark the model using the following metrics:\n",
    "- training time\n",
    "- prediction speed\n",
    "- performance\n",
    "- memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ffcd4629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type:  float64  | Memory Usage:  6400  bytes\n",
      "Training time:  0.08642077445983887  s\n",
      "Prediction speed:  0.0006823539733886719  s\n",
      "Log-Likelihood:  -0.6597161202802081\n",
      "AIC:  411.88644811208326\n",
      "BIC:  655.961933236638\n"
     ]
    }
   ],
   "source": [
    "# The following cell is copied and pasted from the D804_PA_Model_Customer_Segmentation.ipynb notebook\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Mall_Customers.csv')\n",
    "\n",
    "# Encode gender column\n",
    "df['Gender'] = df['Gender'].astype('category').cat.codes\n",
    "# Male = 1, Female = 0\n",
    "# Dropping the ID column as that has little impact on patterns\n",
    "df.drop(['CustomerID'], axis=1, inplace=True)\n",
    "\n",
    "# Scale the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df)\n",
    "print(\"Data type: \", X_scaled.dtype, \" | Memory Usage: \", X_scaled.nbytes, \" bytes\") # Get memory usage\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import time\n",
    "\n",
    "# Base model\n",
    "base_model = GaussianMixture(n_components=5, covariance_type='full', \n",
    "                        reg_covar=0.0001, n_init=5, random_state=42)\n",
    "\n",
    "# Measure training time\n",
    "train_base_start = time.time()\n",
    "base_model.fit(X_scaled)\n",
    "train_base_end = time.time()\n",
    "print(\"Training time: \", train_base_end - train_base_start, \" s\")\n",
    "\n",
    "# Measure prediction speed\n",
    "pred_base_start = time.time()\n",
    "base_labels = base_model.predict(X_scaled)\n",
    "base_probas = base_model.predict_proba(X_scaled)\n",
    "pred_base_end = time.time()\n",
    "print(\"Prediction speed: \", pred_base_end - pred_base_start, \" s\")\n",
    "\n",
    "# Performance\n",
    "print(\"Log-Likelihood: \", base_model.score(X_scaled))\n",
    "print(\"AIC: \", base_model.aic(X_scaled))\n",
    "print(\"BIC: \", base_model.bic(X_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f29db7",
   "metadata": {},
   "source": [
    "### Baseline Benchmarks\n",
    "The baseline model yields the following benchmark metrics:\n",
    "- training time: 0.086s\n",
    "- prediction speed: 0.0007s\n",
    "- performance: Log-likelihood -0.66, AIC 411.89, BIC 655.96\n",
    "- memory usage: scaled data using 6400 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c15dc2",
   "metadata": {},
   "source": [
    "## Model Optimization Process\n",
    "\n",
    "To improve the benchmarks of the current working model, I will employ the following optimization strategies:\n",
    "- efficient numerics representation by changing data types\n",
    "- modify learning dynamics by changing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fba539ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type:  float16  | Memory Usage:  1600  bytes\n",
      "Training time:  0.07280659675598145  s\n",
      "Prediction speed:  0.0007600784301757812  s\n",
      "Log-Likelihood:  -0.4815075219378944\n",
      "AIC:  340.6030087751578\n",
      "BIC:  584.6784938997125\n"
     ]
    }
   ],
   "source": [
    "# Efficient numerics by changing the float64 data types into float32\n",
    "X_scaled = X_scaled.astype(\"float16\")\n",
    "print(\"Data type: \", X_scaled.dtype, \" | Memory Usage: \", X_scaled.nbytes, \" bytes\") # Get memory usage\n",
    "\n",
    "# Modifying model hyperparameters\n",
    "opt_model = GaussianMixture(n_components=5, covariance_type='full', \n",
    "                        reg_covar=0.00007, n_init=5, random_state=42,\n",
    "                        max_iter=90, tol=0.001)\n",
    "# Added new hyperparameters: max_iter and tol\n",
    "\n",
    "# Measure training time\n",
    "train_opt_start = time.time()\n",
    "opt_model.fit(X_scaled)\n",
    "train_opt_end = time.time()\n",
    "print(\"Training time: \", train_opt_end - train_opt_start, \" s\")\n",
    "\n",
    "# Measure prediction speed\n",
    "pred_opt_start = time.time()\n",
    "opt_labels = opt_model.predict(X_scaled)\n",
    "opt_probas = opt_model.predict_proba(X_scaled)\n",
    "pred_opt_end = time.time()\n",
    "print(\"Prediction speed: \", pred_opt_end - pred_opt_start, \" s\")\n",
    "\n",
    "# Performance\n",
    "print(\"Log-Likelihood: \", opt_model.score(X_scaled))\n",
    "print(\"AIC: \", opt_model.aic(X_scaled))\n",
    "print(\"BIC: \", opt_model.bic(X_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b4590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Cluster Assignment Confidence\n",
      "count    200.000000\n",
      "mean       0.981629\n",
      "std        0.070013\n",
      "min        0.504017\n",
      "25%        0.997215\n",
      "50%        0.999951\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "dtype: float64\n",
      "--------------------------------------------------\n",
      "Optimized Cluster Assignment Confidence\n",
      "count    200.000000\n",
      "mean       0.981756\n",
      "std        0.069346\n",
      "min        0.507706\n",
      "25%        0.997243\n",
      "50%        0.999951\n",
      "75%        1.000000\n",
      "max        1.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Compare cluster assignment confidence\n",
    "base_conf = base_probas.max(axis=1)\n",
    "opt_conf = opt_probas.max(axis=1)\n",
    "\n",
    "print(\"Baseline Cluster Assignment Confidence\")\n",
    "print(pd.Series(base_conf).describe())\n",
    "\n",
    "print(\"-\"*50)\n",
    "\n",
    "print(\"Optimized Cluster Assignment Confidence\")\n",
    "print(pd.Series(opt_conf).describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2e30e5",
   "metadata": {},
   "source": [
    "## Model Optimization Actions\n",
    "- reduce feature data type to float16\n",
    "- added new hyperparameters to the model: max_iter and tol\n",
    "    - max_iter: reduce the E-M steps > faster training time\n",
    "    - tol: convergence tolerance to integrate early stopping > faster training time\n",
    "- modified existing hyperparameter: reg_covar\n",
    "    - prevents oversmoothing of covariance matrices > improve model stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc562e0",
   "metadata": {},
   "source": [
    "## Evaluating Optimized Model\n",
    "- training time: 0.073s\n",
    "- prediction speed: 0.0008s\n",
    "- performance: Log-likelihood -0.48, AIC 340.60, BIC 584.68\n",
    "- memory usage: scaled data using 1600 bytes\n",
    "\n",
    "As seen by the results, the optimized model yielded much faster training and prediction speeds and decreased memory usage. Below is a comparison of before and after metrics:\n",
    "- training time: 0.086s >> 0.073s\n",
    "- prediction speed: 0.0007s >> 0.0008s\n",
    "- performance: \n",
    "    - log-likelihood: -0.66 >> -0.48\n",
    "    - AIC: 411.89 >> 340.60\n",
    "    - BIC: 655.96 >> 584.68\n",
    "- memory usage: 6400 bytes >> 1600 bytes\n",
    "- cluster assignment confidence remained consistent between both models, indicating stable predictions\n",
    "\n",
    "I observe that the optimized model's training time is slightly reduced and prediction speed is similar. The model performance has significantly improved, increasing log-likelihood and reducing AIC/BIC scores, indicating better clustering performance. Furthermore, memory reduction transforms 6400 byte numerical data into 1600 bytes, significantly reducing memory usage. The increase in performance and memory usage while maintaing relatively similar speeds defines the success of model optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9dce2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
